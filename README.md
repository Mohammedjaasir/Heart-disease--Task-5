## ğŸŒ³ Heart Disease Prediction - Decision Trees & Random Forests (Task 5)

> This project implements **Decision Tree** and **Random Forest** classifiers to predict heart disease presence using patient data. It explores tree visualization, overfitting control, feature importance, and model evaluation with cross-validation.

---

## ğŸ“ Dataset
- Source: [Heart Disease Dataset (Kaggle)](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset)
- Target: `target` (1 = heart disease, 0 = no heart disease)
- Features: Age, sex, chest pain type, blood pressure, cholesterol, etc.

---

## ğŸ¯ Objectives
- Train and visualize a Decision Tree Classifier
- Prevent overfitting using `max_depth`
- Train a Random Forest and compare performance
- Interpret feature importances
- Evaluate both models using accuracy and cross-validation

---

## ğŸ§° Tools & Libraries
| Tool             | Purpose                           |
|------------------|-----------------------------------|
| Pandas           | Data handling                     |
| Matplotlib/Seaborn | Data visualization              |
| Scikit-learn     | Machine learning models & metrics |

---

## ğŸ”„ Workflow Summary

1. **Data Exploration**
    - Checked data types, nulls, correlations
2. **Preprocessing**
    - Target-feature split, train-test split
3. **Decision Tree**
    - Trained base model, visualized tree
    - Limited depth to reduce overfitting
4. **Random Forest**
    - Compared performance with ensemble method
5. **Feature Importance**
    - Bar chart from Random Forest
6. **Cross-Validation**
    - 5-fold CV to validate performance stability

---

## ğŸ“Š Key Visuals

- âœ… Full Decision Tree Diagram
- ğŸ“‰ Pruned Tree Accuracy Comparison
- ğŸŒ² Random Forest Feature Importance Plot
- ğŸ“‹ Confusion Matrix & Classification Report

---

## ğŸ’¡ Key Insights

- **Decision Trees** are highly interpretable but can overfit.
- **Random Forests** offer better generalization with slightly better accuracy.
- Features like `cp` (chest pain), `thal`, and `ca` were most important.
- Pruning decision trees improved test performance.

---

## ğŸ“‚ Project Structure

ğŸ“ decision-tree-random-forest
â”‚
â”œâ”€â”€ heart.csv # Dataset
â”œâ”€â”€ task5_decision_tree_random_forest.py # Python script
â”œâ”€â”€ README.md # This file

---

## ğŸš€ How to Run
1. Clone this repo
2. Install dependencies:  
   `pip install pandas scikit-learn matplotlib seaborn`
3. Run:  
   `python task5_decision_tree_random_forest.py`

---

## âœ¨ Final Thoughts
This task helps build intuition behind tree-based models â€” how they split, when they overfit, and why ensembles like random forests perform better in real-world tasks.
